{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "799108e8-f347-435e-b56c-78cdb1fab0ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T21:54:17.627041Z",
     "iopub.status.busy": "2025-06-20T21:54:17.626733Z",
     "iopub.status.idle": "2025-06-20T21:54:17.637528Z",
     "shell.execute_reply": "2025-06-20T21:54:17.636977Z",
     "shell.execute_reply.started": "2025-06-20T21:54:17.627020Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from botocore.exceptions import NoCredentialsError, PartialCredentialsError, ClientError\n",
    "\n",
    "# --- Helper Functions (Download) ---\n",
    "\n",
    "def download_file_from_s3(s3_key: str, local_file_path: str):\n",
    "    \"\"\"\n",
    "    Downloads a single file from an S3 bucket to a local path.\n",
    "\n",
    "    Args:\n",
    "        s3_key (str): The S3 object key (path and filename within the bucket).\n",
    "        local_file_path (str): The full local path where the file will be saved.\n",
    "    \"\"\"\n",
    "    print(f\"Attempting to download s3://{s3bucket}/{s3_key} to '{local_file_path}'\")\n",
    "    # Ensure the local directory exists\n",
    "    os.makedirs(os.path.dirname(local_file_path) or '.', exist_ok=True)\n",
    "    try:\n",
    "        s3_client.download_file(s3bucket, s3_key, local_file_path)\n",
    "        print(f\"Successfully downloaded '{s3_key}' to '{local_file_path}'\")\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == \"404\":\n",
    "            print(f\"Error: The S3 object '{s3_key}' was not found in bucket '{s3bucket}'.\")\n",
    "        else:\n",
    "            print(f\"Error downloading file from S3: {e}\")\n",
    "    except (NoCredentialsError, PartialCredentialsError):\n",
    "        print(\"Error: AWS credentials not found or incomplete. \"\n",
    "              \"Please configure your AWS credentials.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during download: {e}\")\n",
    "\n",
    "def download_directory_from_s3(s3_prefix_for_dir: str, local_directory_path: str):\n",
    "    \"\"\"\n",
    "    Downloads all objects under a given S3 prefix (acting like a directory)\n",
    "    to a local directory.\n",
    "\n",
    "    Args:\n",
    "        s3_prefix_for_dir (str): The S3 prefix (folder path) in the bucket.\n",
    "                                 Make sure it ends with a '/' to fetch contents of a \"folder\".\n",
    "        local_directory_path (str): The local directory where files will be saved.\n",
    "    \"\"\"\n",
    "    if not s3_prefix_for_dir.endswith('/'):\n",
    "        s3_prefix_for_dir += '/' # Ensure it's treated as a prefix for a folder\n",
    "\n",
    "    print(f\"Starting download of S3 prefix '{s3_prefix_for_dir}' to local directory '{local_directory_path}'\")\n",
    "    os.makedirs(local_directory_path, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # List objects in the specified S3 prefix\n",
    "        paginator = s3_client.get_paginator('list_objects_v2')\n",
    "        pages = paginator.paginate(Bucket=s3bucket, Prefix=s3_prefix_for_dir)\n",
    "\n",
    "        downloaded_count = 0\n",
    "        for page in pages:\n",
    "            if \"Contents\" in page:\n",
    "                for obj in page[\"Contents\"]:\n",
    "                    s3_key = obj[\"Key\"]\n",
    "                    # Skip if the key is just the prefix itself (empty folder representation)\n",
    "                    if s3_key == s3_prefix_for_dir:\n",
    "                        continue\n",
    "\n",
    "                    # Construct the local file path\n",
    "                    # Remove the base s3_prefix from the s3_key to get relative path\n",
    "                    relative_path = os.path.relpath(s3_key, s3_prefix_for_dir)\n",
    "                    local_file_path = os.path.join(local_directory_path, relative_path)\n",
    "\n",
    "                    # Ensure local subdirectories exist\n",
    "                    os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "\n",
    "                    download_file_from_s3(s3_key, local_file_path)\n",
    "                    downloaded_count += 1\n",
    "        if downloaded_count == 0:\n",
    "            print(f\"No files found under S3 prefix '{s3_prefix_for_dir}' to download.\")\n",
    "        else:\n",
    "            print(f\"Finished downloading {downloaded_count} files from S3 prefix '{s3_prefix_for_dir}'.\")\n",
    "\n",
    "    except ClientError as e:\n",
    "        print(f\"Error listing objects in S3: {e}\")\n",
    "    except (NoCredentialsError, PartialCredentialsError):\n",
    "        print(\"Error: AWS credentials not found or incomplete. \"\n",
    "              \"Please configure your AWS credentials.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during directory download: {e}\")\n",
    "\n",
    "def upload_file_to_s3(local_file_path: str, s3_key: str):\n",
    "    \"\"\"\n",
    "    Uploads a single file from a local path to an S3 bucket.\n",
    "\n",
    "    Args:\n",
    "        local_file_path (str): The full path to the local file.\n",
    "        s3_key (str): The S3 object key (path and filename within the bucket).\n",
    "                      This typically includes the s3prefix.\n",
    "    \"\"\"\n",
    "    print(f\"Attempting to upload '{local_file_path}' to s3://{s3bucket}/{s3_key}\")\n",
    "    try:\n",
    "        s3_client.upload_file(local_file_path, s3bucket, s3_key)\n",
    "        print(f\"Successfully uploaded '{local_file_path}' to S3 at '{s3_key}'\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{local_file_path}' was not found.\")\n",
    "    except (NoCredentialsError, PartialCredentialsError):\n",
    "        print(\"Error: AWS credentials not found or incomplete. \"\n",
    "              \"Please configure your AWS credentials.\")\n",
    "    except ClientError as e:\n",
    "        print(f\"Error uploading file to S3: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "def upload_directory_to_s3(local_directory_path: str, s3_prefix_for_dir: str):\n",
    "    \"\"\"\n",
    "    Uploads an entire directory (and its contents recursively) to an S3 bucket.\n",
    "\n",
    "    Args:\n",
    "        local_directory_path (str): The full path to the local directory.\n",
    "        s3_prefix_for_dir (str): The S3 prefix (folder path) within the bucket\n",
    "                                 where the directory contents will be stored.\n",
    "                                 Make sure it ends with a '/' if it represents a folder.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(local_directory_path):\n",
    "        print(f\"Error: '{local_directory_path}' is not a valid directory.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Starting upload of directory '{local_directory_path}' to s3://{s3bucket}/{s3_prefix_for_dir}\")\n",
    "\n",
    "    for root, dirs, files in os.walk(local_directory_path):\n",
    "        for file in files:\n",
    "            local_file_path = os.path.join(root, file)\n",
    "            # Construct the S3 key relative to the base local_directory_path\n",
    "            relative_path = os.path.relpath(local_file_path, local_directory_path)\n",
    "            s3_key = os.path.join(s3_prefix_for_dir, relative_path).replace(\"\\\\\", \"/\") # Ensure forward slashes for S3\n",
    "\n",
    "            upload_file_to_s3(local_file_path, s3_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aca59d4-6886-4bd9-bbf6-7120ea2ca5f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T21:39:10.949638Z",
     "iopub.status.busy": "2025-06-20T21:39:10.949267Z",
     "iopub.status.idle": "2025-06-20T21:39:11.047002Z",
     "shell.execute_reply": "2025-06-20T21:39:11.046485Z",
     "shell.execute_reply.started": "2025-06-20T21:39:10.949616Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# Replace with your S3 bucket name\n",
    "team = 'model-risk'\n",
    "account = 'sagemakerprod'\n",
    "project_name='ARM1_2025'\n",
    "s3prefix = f\"{project_name}\"\n",
    "s3bucket = f\"upg-sagemaker-{team}-usw2-{account}\"\n",
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60588e02-5c3f-4bc9-92d6-c6787a6dae63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T21:55:33.868043Z",
     "iopub.status.busy": "2025-06-20T21:55:33.867728Z",
     "iopub.status.idle": "2025-06-20T21:55:34.305059Z",
     "shell.execute_reply": "2025-06-20T21:55:34.304581Z",
     "shell.execute_reply.started": "2025-06-20T21:55:33.868023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to download s3://upg-sagemaker-model-risk-usw2-sagemakerprod/ARM1_2025/sample_df.dat to 'data/sample_df.dat'\n",
      "Successfully downloaded 'ARM1_2025/sample_df.dat' to 'data/sample_df.dat'\n"
     ]
    }
   ],
   "source": [
    "#Download single file to Sagemaker from S3\n",
    "s3_download_file_key = os.path.join(s3prefix, 'sample_df.dat').replace(\"\\\\\", \"/\")\n",
    "local_download_file_path = \"data/sample_df.dat\"\n",
    "download_file_from_s3(s3_download_file_key, local_download_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9badc0df-007a-4667-a6c5-da1045743657",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T21:52:20.361464Z",
     "iopub.status.busy": "2025-06-20T21:52:20.360922Z",
     "iopub.status.idle": "2025-06-20T21:52:20.364903Z",
     "shell.execute_reply": "2025-06-20T21:52:20.364394Z",
     "shell.execute_reply.started": "2025-06-20T21:52:20.361442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARM1_2025/\n"
     ]
    }
   ],
   "source": [
    "#Download a folder to SM from S3\n",
    "sub_dir=''\n",
    "s3_download_dir_prefix = os.path.join(s3prefix, os.path.basename(sub_dir), \"\").replace(\"\\\\\", \"/\")\n",
    "print(s3_download_dir_prefix)\n",
    "\n",
    "#local_download_dir_path = \"data/\"\n",
    "#download_directory_from_s3(s3_download_dir_prefix, local_download_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "405325e1-79de-4ba7-9537-9fc1e0cb12f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T21:55:08.487098Z",
     "iopub.status.busy": "2025-06-20T21:55:08.486567Z",
     "iopub.status.idle": "2025-06-20T21:55:08.567775Z",
     "shell.execute_reply": "2025-06-20T21:55:08.567281Z",
     "shell.execute_reply.started": "2025-06-20T21:55:08.487072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to upload 'data/test_final_mrm.dat' to s3://upg-sagemaker-model-risk-usw2-sagemakerprod/ARM1_2025/data/test_final_mrm.dat\n",
      "Successfully uploaded 'data/test_final_mrm.dat' to S3 at 'ARM1_2025/data/test_final_mrm.dat'\n"
     ]
    }
   ],
   "source": [
    "#Upload a single file to S3 from SM\n",
    "data_dir='data/'\n",
    "dummy_file_name = data_dir+\"test_final_mrm.dat\"\n",
    "\n",
    "# Define the S3 key for the single file\n",
    "s3_file_key = os.path.join(s3prefix, dummy_file_name).replace(\"\\\\\", \"/\")\n",
    "upload_file_to_s3(dummy_file_name, s3_file_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff37d861-4e9e-4ed6-b539-48be7422b825",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload a folder to S3\n",
    "data_dir = \"data/\"\n",
    "\n",
    "# Define the S3 prefix for the directory upload\n",
    "s3_dir_prefix = os.path.join(s3prefix, os.path.basename(data_dir), \"\").replace(\"\\\\\", \"/\")\n",
    "upload_directory_to_s3(dummy_dir, s3_dir_prefix)\n",
    "\n",
    "## Clean up the directory if you want to\n",
    "#import shutil\n",
    "#shutil.rmtree(dummy_dir)\n",
    "#print(f\"Cleaned up '{dummy_dir}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
